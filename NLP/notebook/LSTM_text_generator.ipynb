{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbC55OXDqg0d",
        "outputId": "8eae6b9f-0fa9-48f0-f439-3d0a5f7ffa58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aashita/nyt-comments?dataset_version_number=13...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 480M/480M [00:23<00:00, 21.0MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/aashita/nyt-comments/versions/13\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"aashita/nyt-comments\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RjyQxtzyqkq2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkOSstZCq1Wm",
        "outputId": "f1e9d3f4-42c1-4196-b217-7315edfd31ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
              "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
              "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv('/root/.cache/kagglehub/datasets/aashita/nyt-comments/versions/13/ArticlesApril2017.csv')\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lL32m4zAq_qT"
      },
      "outputs": [],
      "source": [
        "class TextGeneration(Dataset):\n",
        "  '''\n",
        "  self.BOW : 문서 전체에 대한 사전, {'ABC' : 123, 'EEG' :556}\n",
        "  self.corpus : text lines, ['ghsldhsidg','sdfnsldfnunvls']\n",
        "  self.data : 텍스트 한줄 을 모델에 input할 때 format,corpus 한줄에 해당하는 BOW의 value값 [([0,1],2),([1,2],3),([2,3],4),([3,4],5),([4,5],6)]\n",
        "  '''\n",
        "  def clean_text(self,txt):\n",
        "    txt= \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    return txt\n",
        "\n",
        "  def __init__(self):\n",
        "    all_headlines=[]\n",
        "\n",
        "    for filename in glob.glob('/root/.cache/kagglehub/datasets/aashita/nyt-comments/versions/13/*.csv'):\n",
        "        if 'Articles' in filename:\n",
        "          article_df=pd.read_csv(filename)\n",
        "          all_headlines.extend(list(article_df.headline.values))\n",
        "          break\n",
        "\n",
        "    all_headlines=[h for h in all_headlines if h!='Unknown']\n",
        "\n",
        "    self.corpus=[self.clean_text(x) for x in all_headlines]\n",
        "    self.BOW={}\n",
        "\n",
        "    for line in self.corpus:\n",
        "      for word in line.split():\n",
        "        if word not in self.BOW.keys():\n",
        "          self.BOW[word]=len(self.BOW.keys())\n",
        "\n",
        "    self.data=self.generate_sequence(self.corpus)\n",
        "\n",
        "  def generate_sequence(self,txt):\n",
        "    seq=[]\n",
        "\n",
        "    for line in txt:\n",
        "      line=line.split()\n",
        "      line_bow=[self.BOW[word] for word in line]\n",
        "\n",
        "      data=[([line_bow[i], line_bow[i+1]],line_bow[i+2])\n",
        "            for i in range(len(line_bow)-2)]\n",
        "\n",
        "      seq.extend(data)\n",
        "      return seq\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    data=np.array(self.data[i][0])\n",
        "    label=np.array(self.data[i][1]).astype(np.float32)\n",
        "    return data,label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iGjsDnCnrAVa"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_embeddings):\n",
        "    super(LSTM,self).__init__()\n",
        "\n",
        "    self.embed=nn.Embedding(num_embeddings=num_embeddings,embedding_dim=16)\n",
        "\n",
        "    self.lstm=nn.LSTM(\n",
        "        input_size=16,\n",
        "        hidden_size=64,\n",
        "        num_layers=5,\n",
        "        batch_first=True)\n",
        "\n",
        "\n",
        "    self.fc1=nn.Linear(128,num_embeddings)\n",
        "    self.fc2=nn.Linear(num_embeddings,num_embeddings)\n",
        "\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.embed(x)\n",
        "\n",
        "    x,_=self.lstm(x)\n",
        "    x=torch.reshape(x,(x.shape[0],-1))\n",
        "    x=self.fc1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EalFIrpy5MS",
        "outputId": "8812eb81-ebb2-40d7-9fdd-2577aafca73e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-30-a0b9598ce538>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred=model(torch.tensor(data,dtype=torch.long).to(device))\n",
            "<ipython-input-30-a0b9598ce538>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss=nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype=torch.long).to(device))\n",
            "epoch0 loss:7.7467942237854: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
            "epoch1 loss:7.619811058044434: 100%|██████████| 1/1 [00:00<00:00, 120.58it/s]\n",
            "epoch2 loss:7.490128517150879: 100%|██████████| 1/1 [00:00<00:00, 96.31it/s]\n",
            "epoch3 loss:7.352680206298828: 100%|██████████| 1/1 [00:00<00:00, 105.46it/s]\n",
            "epoch4 loss:7.202883243560791: 100%|██████████| 1/1 [00:00<00:00, 101.32it/s]\n",
            "epoch5 loss:7.036068916320801: 100%|██████████| 1/1 [00:00<00:00, 101.09it/s]\n",
            "epoch6 loss:6.847692012786865: 100%|██████████| 1/1 [00:00<00:00, 93.90it/s]\n",
            "epoch7 loss:6.633042335510254: 100%|██████████| 1/1 [00:00<00:00, 106.18it/s]\n",
            "epoch8 loss:6.387450218200684: 100%|██████████| 1/1 [00:00<00:00, 102.55it/s]\n",
            "epoch9 loss:6.105827331542969: 100%|██████████| 1/1 [00:00<00:00, 98.80it/s]\n",
            "epoch10 loss:5.782652854919434: 100%|██████████| 1/1 [00:00<00:00, 100.51it/s]\n",
            "epoch11 loss:5.4121294021606445: 100%|██████████| 1/1 [00:00<00:00, 101.45it/s]\n",
            "epoch12 loss:4.9884443283081055: 100%|██████████| 1/1 [00:00<00:00, 106.40it/s]\n",
            "epoch13 loss:4.506659507751465: 100%|██████████| 1/1 [00:00<00:00, 112.20it/s]\n",
            "epoch14 loss:3.9657416343688965: 100%|██████████| 1/1 [00:00<00:00, 110.03it/s]\n",
            "epoch15 loss:3.375962018966675: 100%|██████████| 1/1 [00:00<00:00, 118.76it/s]\n",
            "epoch16 loss:2.7737536430358887: 100%|██████████| 1/1 [00:00<00:00, 114.87it/s]\n",
            "epoch17 loss:2.2399404048919678: 100%|██████████| 1/1 [00:00<00:00, 98.80it/s]\n",
            "epoch18 loss:1.8742587566375732: 100%|██████████| 1/1 [00:00<00:00, 115.56it/s]\n",
            "epoch19 loss:1.6971172094345093: 100%|██████████| 1/1 [00:00<00:00, 120.38it/s]\n",
            "epoch20 loss:1.6344022750854492: 100%|██████████| 1/1 [00:00<00:00, 111.54it/s]\n",
            "epoch21 loss:1.615980863571167: 100%|██████████| 1/1 [00:00<00:00, 102.89it/s]\n",
            "epoch22 loss:1.6107819080352783: 100%|██████████| 1/1 [00:00<00:00, 101.52it/s]\n",
            "epoch23 loss:1.6091172695159912: 100%|██████████| 1/1 [00:00<00:00, 88.10it/s]\n",
            "epoch24 loss:1.6086422204971313: 100%|██████████| 1/1 [00:00<00:00, 96.22it/s]\n",
            "epoch25 loss:1.6086641550064087: 100%|██████████| 1/1 [00:00<00:00, 72.38it/s]\n",
            "epoch26 loss:1.608630895614624: 100%|██████████| 1/1 [00:00<00:00, 84.76it/s]\n",
            "epoch27 loss:1.6078828573226929: 100%|██████████| 1/1 [00:00<00:00, 86.94it/s]\n",
            "epoch28 loss:1.6065313816070557: 100%|██████████| 1/1 [00:00<00:00, 107.28it/s]\n",
            "epoch29 loss:1.6053413152694702: 100%|██████████| 1/1 [00:00<00:00, 92.45it/s]\n",
            "epoch30 loss:1.6046339273452759: 100%|██████████| 1/1 [00:00<00:00, 67.89it/s]\n",
            "epoch31 loss:1.6036739349365234: 100%|██████████| 1/1 [00:00<00:00, 66.22it/s]\n",
            "epoch32 loss:1.6015679836273193: 100%|██████████| 1/1 [00:00<00:00, 63.62it/s]\n",
            "epoch33 loss:1.5989863872528076: 100%|██████████| 1/1 [00:00<00:00, 101.05it/s]\n",
            "epoch34 loss:1.5968194007873535: 100%|██████████| 1/1 [00:00<00:00, 73.62it/s]\n",
            "epoch35 loss:1.5938551425933838: 100%|██████████| 1/1 [00:00<00:00, 89.19it/s]\n",
            "epoch36 loss:1.5893371105194092: 100%|██████████| 1/1 [00:00<00:00, 99.42it/s]\n",
            "epoch37 loss:1.5843660831451416: 100%|██████████| 1/1 [00:00<00:00, 75.25it/s]\n",
            "epoch38 loss:1.578578233718872: 100%|██████████| 1/1 [00:00<00:00, 83.75it/s]\n",
            "epoch39 loss:1.5706716775894165: 100%|██████████| 1/1 [00:00<00:00, 98.27it/s]\n",
            "epoch40 loss:1.5615400075912476: 100%|██████████| 1/1 [00:00<00:00, 98.26it/s]\n",
            "epoch41 loss:1.550769329071045: 100%|██████████| 1/1 [00:00<00:00, 88.50it/s]\n",
            "epoch42 loss:1.5370196104049683: 100%|██████████| 1/1 [00:00<00:00, 88.36it/s]\n",
            "epoch43 loss:1.5216363668441772: 100%|██████████| 1/1 [00:00<00:00, 88.51it/s]\n",
            "epoch44 loss:1.5030524730682373: 100%|██████████| 1/1 [00:00<00:00, 74.21it/s]\n",
            "epoch45 loss:1.4817943572998047: 100%|██████████| 1/1 [00:00<00:00, 89.15it/s]\n",
            "epoch46 loss:1.4566742181777954: 100%|██████████| 1/1 [00:00<00:00, 113.38it/s]\n",
            "epoch47 loss:1.4281063079833984: 100%|██████████| 1/1 [00:00<00:00, 96.12it/s]\n",
            "epoch48 loss:1.3956633806228638: 100%|██████████| 1/1 [00:00<00:00, 97.73it/s]\n",
            "epoch49 loss:1.3594707250595093: 100%|██████████| 1/1 [00:00<00:00, 81.33it/s]\n",
            "epoch50 loss:1.3187780380249023: 100%|██████████| 1/1 [00:00<00:00, 88.95it/s]\n",
            "epoch51 loss:1.2749559879302979: 100%|██████████| 1/1 [00:00<00:00, 69.72it/s]\n",
            "epoch52 loss:1.2283128499984741: 100%|██████████| 1/1 [00:00<00:00, 69.90it/s]\n",
            "epoch53 loss:1.180174708366394: 100%|██████████| 1/1 [00:00<00:00, 95.76it/s]\n",
            "epoch54 loss:1.1326329708099365: 100%|██████████| 1/1 [00:00<00:00, 82.87it/s]\n",
            "epoch55 loss:1.0874054431915283: 100%|██████████| 1/1 [00:00<00:00, 78.77it/s]\n",
            "epoch56 loss:1.0455796718597412: 100%|██████████| 1/1 [00:00<00:00, 81.18it/s]\n",
            "epoch57 loss:1.0082377195358276: 100%|██████████| 1/1 [00:00<00:00, 82.42it/s]\n",
            "epoch58 loss:0.9761689305305481: 100%|██████████| 1/1 [00:00<00:00, 69.35it/s]\n",
            "epoch59 loss:0.951407790184021: 100%|██████████| 1/1 [00:00<00:00, 81.17it/s]\n",
            "epoch60 loss:0.9304448962211609: 100%|██████████| 1/1 [00:00<00:00, 77.94it/s]\n",
            "epoch61 loss:0.8983699083328247: 100%|██████████| 1/1 [00:00<00:00, 81.41it/s]\n",
            "epoch62 loss:0.866422176361084: 100%|██████████| 1/1 [00:00<00:00, 89.71it/s]\n",
            "epoch63 loss:0.8430258631706238: 100%|██████████| 1/1 [00:00<00:00, 82.08it/s]\n",
            "epoch64 loss:0.8114238977432251: 100%|██████████| 1/1 [00:00<00:00, 90.58it/s]\n",
            "epoch65 loss:0.7765328288078308: 100%|██████████| 1/1 [00:00<00:00, 86.03it/s]\n",
            "epoch66 loss:0.7445381283760071: 100%|██████████| 1/1 [00:00<00:00, 84.21it/s]\n",
            "epoch67 loss:0.7046202421188354: 100%|██████████| 1/1 [00:00<00:00, 86.70it/s]\n",
            "epoch68 loss:0.6695382595062256: 100%|██████████| 1/1 [00:00<00:00, 109.92it/s]\n",
            "epoch69 loss:0.6323822736740112: 100%|██████████| 1/1 [00:00<00:00, 107.85it/s]\n",
            "epoch70 loss:0.5957587957382202: 100%|██████████| 1/1 [00:00<00:00, 109.52it/s]\n",
            "epoch71 loss:0.5661656856536865: 100%|██████████| 1/1 [00:00<00:00, 86.41it/s]\n",
            "epoch72 loss:0.538183867931366: 100%|██████████| 1/1 [00:00<00:00, 91.25it/s]\n",
            "epoch73 loss:0.5122945308685303: 100%|██████████| 1/1 [00:00<00:00, 94.80it/s]\n",
            "epoch74 loss:0.48215222358703613: 100%|██████████| 1/1 [00:00<00:00, 87.73it/s]\n",
            "epoch75 loss:0.4598987102508545: 100%|██████████| 1/1 [00:00<00:00, 98.86it/s]\n",
            "epoch76 loss:0.43472322821617126: 100%|██████████| 1/1 [00:00<00:00, 78.58it/s]\n",
            "epoch77 loss:0.41272562742233276: 100%|██████████| 1/1 [00:00<00:00, 84.23it/s]\n",
            "epoch78 loss:0.39367300271987915: 100%|██████████| 1/1 [00:00<00:00, 80.67it/s]\n",
            "epoch79 loss:0.3745673596858978: 100%|██████████| 1/1 [00:00<00:00, 102.21it/s]\n",
            "epoch80 loss:0.3581070005893707: 100%|██████████| 1/1 [00:00<00:00, 70.10it/s]\n",
            "epoch81 loss:0.34601524472236633: 100%|██████████| 1/1 [00:00<00:00, 89.99it/s]\n",
            "epoch82 loss:0.33498263359069824: 100%|██████████| 1/1 [00:00<00:00, 89.29it/s]\n",
            "epoch83 loss:0.32576823234558105: 100%|██████████| 1/1 [00:00<00:00, 87.76it/s]\n",
            "epoch84 loss:0.3190116584300995: 100%|██████████| 1/1 [00:00<00:00, 106.89it/s]\n",
            "epoch85 loss:0.31272098422050476: 100%|██████████| 1/1 [00:00<00:00, 69.01it/s]\n",
            "epoch86 loss:0.30648937821388245: 100%|██████████| 1/1 [00:00<00:00, 98.20it/s]\n",
            "epoch87 loss:0.30097275972366333: 100%|██████████| 1/1 [00:00<00:00, 66.71it/s]\n",
            "epoch88 loss:0.29562777280807495: 100%|██████████| 1/1 [00:00<00:00, 74.78it/s]\n",
            "epoch89 loss:0.2902992367744446: 100%|██████████| 1/1 [00:00<00:00, 85.71it/s]\n",
            "epoch90 loss:0.28509774804115295: 100%|██████████| 1/1 [00:00<00:00, 90.95it/s]\n",
            "epoch91 loss:0.2798391282558441: 100%|██████████| 1/1 [00:00<00:00, 87.22it/s]\n",
            "epoch92 loss:0.27388423681259155: 100%|██████████| 1/1 [00:00<00:00, 97.63it/s]\n",
            "epoch93 loss:0.267090380191803: 100%|██████████| 1/1 [00:00<00:00, 97.45it/s]\n",
            "epoch94 loss:0.25994065403938293: 100%|██████████| 1/1 [00:00<00:00, 74.08it/s]\n",
            "epoch95 loss:0.2524231970310211: 100%|██████████| 1/1 [00:00<00:00, 90.83it/s]\n",
            "epoch96 loss:0.24421580135822296: 100%|██████████| 1/1 [00:00<00:00, 91.56it/s]\n",
            "epoch97 loss:0.2350350320339203: 100%|██████████| 1/1 [00:00<00:00, 68.69it/s]\n",
            "epoch98 loss:0.224512979388237: 100%|██████████| 1/1 [00:00<00:00, 86.21it/s]\n",
            "epoch99 loss:0.21252849698066711: 100%|██████████| 1/1 [00:00<00:00, 97.17it/s]\n",
            "epoch100 loss:0.19870254397392273: 100%|██████████| 1/1 [00:00<00:00, 92.41it/s]\n",
            "epoch101 loss:0.1825791895389557: 100%|██████████| 1/1 [00:00<00:00, 87.40it/s]\n",
            "epoch102 loss:0.16441312432289124: 100%|██████████| 1/1 [00:00<00:00, 82.88it/s]\n",
            "epoch103 loss:0.14446236193180084: 100%|██████████| 1/1 [00:00<00:00, 81.97it/s]\n",
            "epoch104 loss:0.12306690216064453: 100%|██████████| 1/1 [00:00<00:00, 77.66it/s]\n",
            "epoch105 loss:0.10151553153991699: 100%|██████████| 1/1 [00:00<00:00, 85.04it/s]\n",
            "epoch106 loss:0.08181102573871613: 100%|██████████| 1/1 [00:00<00:00, 76.88it/s]\n",
            "epoch107 loss:0.06620989739894867: 100%|██████████| 1/1 [00:00<00:00, 71.90it/s]\n",
            "epoch108 loss:0.05512151122093201: 100%|██████████| 1/1 [00:00<00:00, 75.28it/s]\n",
            "epoch109 loss:0.04592565447092056: 100%|██████████| 1/1 [00:00<00:00, 79.52it/s]\n",
            "epoch110 loss:0.037861429154872894: 100%|██████████| 1/1 [00:00<00:00, 91.71it/s]\n",
            "epoch111 loss:0.03381868824362755: 100%|██████████| 1/1 [00:00<00:00, 87.51it/s]\n",
            "epoch112 loss:0.02871232107281685: 100%|██████████| 1/1 [00:00<00:00, 83.92it/s]\n",
            "epoch113 loss:0.023715447634458542: 100%|██████████| 1/1 [00:00<00:00, 105.95it/s]\n",
            "epoch114 loss:0.020018521696329117: 100%|██████████| 1/1 [00:00<00:00, 86.09it/s]\n",
            "epoch115 loss:0.01943111978471279: 100%|██████████| 1/1 [00:00<00:00, 87.26it/s]\n",
            "epoch116 loss:0.01616549864411354: 100%|██████████| 1/1 [00:00<00:00, 104.27it/s]\n",
            "epoch117 loss:0.01367777120321989: 100%|██████████| 1/1 [00:00<00:00, 62.17it/s]\n",
            "epoch118 loss:0.011785238049924374: 100%|██████████| 1/1 [00:00<00:00, 74.96it/s]\n",
            "epoch119 loss:0.009497055783867836: 100%|██████████| 1/1 [00:00<00:00, 91.56it/s]\n",
            "epoch120 loss:0.007449702825397253: 100%|██████████| 1/1 [00:00<00:00, 92.25it/s]\n",
            "epoch121 loss:0.006289615295827389: 100%|██████████| 1/1 [00:00<00:00, 100.17it/s]\n",
            "epoch122 loss:0.005783416330814362: 100%|██████████| 1/1 [00:00<00:00, 89.83it/s]\n",
            "epoch123 loss:0.005360565148293972: 100%|██████████| 1/1 [00:00<00:00, 63.23it/s]\n",
            "epoch124 loss:0.004831327125430107: 100%|██████████| 1/1 [00:00<00:00, 76.10it/s]\n",
            "epoch125 loss:0.0043159592896699905: 100%|██████████| 1/1 [00:00<00:00, 100.63it/s]\n",
            "epoch126 loss:0.0038973544724285603: 100%|██████████| 1/1 [00:00<00:00, 81.68it/s]\n",
            "epoch127 loss:0.003599890973418951: 100%|██████████| 1/1 [00:00<00:00, 59.87it/s]\n",
            "epoch128 loss:0.0033941096626222134: 100%|██████████| 1/1 [00:00<00:00, 87.15it/s]\n",
            "epoch129 loss:0.003211966948583722: 100%|██████████| 1/1 [00:00<00:00, 102.41it/s]\n",
            "epoch130 loss:0.0029818073380738497: 100%|██████████| 1/1 [00:00<00:00, 92.11it/s]\n",
            "epoch131 loss:0.00270351255312562: 100%|██████████| 1/1 [00:00<00:00, 85.33it/s]\n",
            "epoch132 loss:0.0024300396908074617: 100%|██████████| 1/1 [00:00<00:00, 106.99it/s]\n",
            "epoch133 loss:0.0022092859726399183: 100%|██████████| 1/1 [00:00<00:00, 110.91it/s]\n",
            "epoch134 loss:0.0020586736500263214: 100%|██████████| 1/1 [00:00<00:00, 72.00it/s]\n",
            "epoch135 loss:0.001957210712134838: 100%|██████████| 1/1 [00:00<00:00, 89.03it/s]\n",
            "epoch136 loss:0.001876247813925147: 100%|██████████| 1/1 [00:00<00:00, 75.10it/s]\n",
            "epoch137 loss:0.00179169955663383: 100%|██████████| 1/1 [00:00<00:00, 78.29it/s]\n",
            "epoch138 loss:0.0016921277856454253: 100%|██████████| 1/1 [00:00<00:00, 80.55it/s]\n",
            "epoch139 loss:0.0015825884183868766: 100%|██████████| 1/1 [00:00<00:00, 73.53it/s]\n",
            "epoch140 loss:0.0014779611956328154: 100%|██████████| 1/1 [00:00<00:00, 87.41it/s]\n",
            "epoch141 loss:0.0013870678376406431: 100%|██████████| 1/1 [00:00<00:00, 85.39it/s]\n",
            "epoch142 loss:0.001312578096985817: 100%|██████████| 1/1 [00:00<00:00, 70.08it/s]\n",
            "epoch143 loss:0.00125157181173563: 100%|██████████| 1/1 [00:00<00:00, 80.96it/s]\n",
            "epoch144 loss:0.001199175021611154: 100%|██████████| 1/1 [00:00<00:00, 92.65it/s]\n",
            "epoch145 loss:0.0011504635913297534: 100%|██████████| 1/1 [00:00<00:00, 78.75it/s]\n",
            "epoch146 loss:0.0011031532194465399: 100%|██████████| 1/1 [00:00<00:00, 93.09it/s]\n",
            "epoch147 loss:0.0010567682329565287: 100%|██████████| 1/1 [00:00<00:00, 95.15it/s]\n",
            "epoch148 loss:0.0010114277247339487: 100%|██████████| 1/1 [00:00<00:00, 88.80it/s]\n",
            "epoch149 loss:0.0009667754056863487: 100%|██████████| 1/1 [00:00<00:00, 83.38it/s]\n",
            "epoch150 loss:0.0009233593009412289: 100%|██████████| 1/1 [00:00<00:00, 72.91it/s]\n",
            "epoch151 loss:0.0008816317422315478: 100%|██████████| 1/1 [00:00<00:00, 89.76it/s]\n",
            "epoch152 loss:0.0008429268491454422: 100%|██████████| 1/1 [00:00<00:00, 99.11it/s]\n",
            "epoch153 loss:0.0008062443812377751: 100%|██████████| 1/1 [00:00<00:00, 63.42it/s]\n",
            "epoch154 loss:0.0007708223420195282: 100%|██████████| 1/1 [00:00<00:00, 82.11it/s]\n",
            "epoch155 loss:0.0007373992120847106: 100%|██████████| 1/1 [00:00<00:00, 94.32it/s]\n",
            "epoch156 loss:0.0007061659707687795: 100%|██████████| 1/1 [00:00<00:00, 88.42it/s]\n",
            "epoch157 loss:0.0006775754736736417: 100%|██████████| 1/1 [00:00<00:00, 86.63it/s]\n",
            "epoch158 loss:0.0006500558811239898: 100%|██████████| 1/1 [00:00<00:00, 90.43it/s]\n",
            "epoch159 loss:0.0006236312328837812: 100%|██████████| 1/1 [00:00<00:00, 73.92it/s]\n",
            "epoch160 loss:0.0005989692872390151: 100%|██████████| 1/1 [00:00<00:00, 86.66it/s]\n",
            "epoch161 loss:0.0005754510639235377: 100%|██████████| 1/1 [00:00<00:00, 90.12it/s]\n",
            "epoch162 loss:0.0005524333100765944: 100%|██████████| 1/1 [00:00<00:00, 100.67it/s]\n",
            "epoch163 loss:0.0005300826160237193: 100%|██████████| 1/1 [00:00<00:00, 75.30it/s]\n",
            "epoch164 loss:0.000508994678966701: 100%|██████████| 1/1 [00:00<00:00, 94.62it/s]\n",
            "epoch165 loss:0.00048824012628756464: 100%|██████████| 1/1 [00:00<00:00, 94.91it/s]\n",
            "epoch166 loss:0.00046886736527085304: 100%|██████████| 1/1 [00:00<00:00, 72.45it/s]\n",
            "epoch167 loss:0.0004504712705966085: 100%|██████████| 1/1 [00:00<00:00, 73.57it/s]\n",
            "epoch168 loss:0.00043250370072200894: 100%|██████████| 1/1 [00:00<00:00, 98.42it/s]\n",
            "epoch169 loss:0.000415012298617512: 100%|██████████| 1/1 [00:00<00:00, 76.03it/s]\n",
            "epoch170 loss:0.00039830702007748187: 100%|██████████| 1/1 [00:00<00:00, 86.32it/s]\n",
            "epoch171 loss:0.00038260233122855425: 100%|██████████| 1/1 [00:00<00:00, 74.47it/s]\n",
            "epoch172 loss:0.00036830344470217824: 100%|██████████| 1/1 [00:00<00:00, 80.06it/s]\n",
            "epoch173 loss:0.0003546714724507183: 100%|██████████| 1/1 [00:00<00:00, 67.56it/s]\n",
            "epoch174 loss:0.00034165900433436036: 100%|██████████| 1/1 [00:00<00:00, 61.22it/s]\n",
            "epoch175 loss:0.0003288607986178249: 100%|██████████| 1/1 [00:00<00:00, 87.45it/s]\n",
            "epoch176 loss:0.0003165390808135271: 100%|██████████| 1/1 [00:00<00:00, 82.47it/s]\n",
            "epoch177 loss:0.00030476535903289914: 100%|██████████| 1/1 [00:00<00:00, 112.39it/s]\n",
            "epoch178 loss:0.00029325371724553406: 100%|██████████| 1/1 [00:00<00:00, 97.37it/s]\n",
            "epoch179 loss:0.00028240919345989823: 100%|██████████| 1/1 [00:00<00:00, 86.16it/s]\n",
            "epoch180 loss:0.0002721365890465677: 100%|██████████| 1/1 [00:00<00:00, 101.73it/s]\n",
            "epoch181 loss:0.00026234047254547477: 100%|██████████| 1/1 [00:00<00:00, 89.41it/s]\n",
            "epoch182 loss:0.00025275879306718707: 100%|██████████| 1/1 [00:00<00:00, 90.93it/s]\n",
            "epoch183 loss:0.0002435345231788233: 100%|██████████| 1/1 [00:00<00:00, 87.97it/s]\n",
            "epoch184 loss:0.00023483452969230711: 100%|██████████| 1/1 [00:00<00:00, 86.64it/s]\n",
            "epoch185 loss:0.00022673033527098596: 100%|██████████| 1/1 [00:00<00:00, 81.53it/s]\n",
            "epoch186 loss:0.00021891210053581744: 100%|██████████| 1/1 [00:00<00:00, 83.04it/s]\n",
            "epoch187 loss:0.0002114751550834626: 100%|██████████| 1/1 [00:00<00:00, 91.68it/s]\n",
            "epoch188 loss:0.00020425273396540433: 100%|██████████| 1/1 [00:00<00:00, 79.82it/s]\n",
            "epoch189 loss:0.00019729250925593078: 100%|██████████| 1/1 [00:00<00:00, 91.73it/s]\n",
            "epoch190 loss:0.00019059446640312672: 100%|██████████| 1/1 [00:00<00:00, 26.57it/s]\n",
            "epoch191 loss:0.000184277756488882: 100%|██████████| 1/1 [00:00<00:00, 81.14it/s]\n",
            "epoch192 loss:0.00017808018310461193: 100%|██████████| 1/1 [00:00<00:00, 79.59it/s]\n",
            "epoch193 loss:0.00017226397176273167: 100%|██████████| 1/1 [00:00<00:00, 83.78it/s]\n",
            "epoch194 loss:0.0001667814067332074: 100%|██████████| 1/1 [00:00<00:00, 89.66it/s]\n",
            "epoch195 loss:0.00016144185792654753: 100%|██████████| 1/1 [00:00<00:00, 90.34it/s]\n",
            "epoch196 loss:0.0001562929683132097: 100%|██████████| 1/1 [00:00<00:00, 75.31it/s]\n",
            "epoch197 loss:0.0001513586175860837: 100%|██████████| 1/1 [00:00<00:00, 98.03it/s]\n",
            "epoch198 loss:0.0001466387475375086: 100%|██████████| 1/1 [00:00<00:00, 85.10it/s]\n",
            "epoch199 loss:0.00014210958033800125: 100%|██████████| 1/1 [00:00<00:00, 91.08it/s]\n"
          ]
        }
      ],
      "source": [
        "device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset=TextGeneration()\n",
        "model=LSTM(num_embeddings=len(dataset.BOW)).to(device) #사전(BOW)의 길이만큼 embedding, BOW가 one-hot encoding 처럼 0이 많은 희소 행렬이기 때문에 임베딩 적용\n",
        "loader=DataLoader(dataset,batch_size=64)\n",
        "optim=Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "  iterator=tqdm.tqdm(loader)\n",
        "  for data, label in iterator:\n",
        "    optim.zero_grad()\n",
        "\n",
        "    pred=model(torch.tensor(data,dtype=torch.long).to(device))\n",
        "\n",
        "    loss=nn.CrossEntropyLoss()(pred,torch.tensor(label,dtype=torch.long).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    iterator.set_description(f'epoch{epoch} loss:{loss.item()}')\n",
        "\n",
        "torch.save(model.state_dict(),'lstm.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2ufaWEk0x17",
        "outputId": "93a0b9bf-fbe5-440b-c3d1-846191bddb7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input word : finding an \n",
            "predicted sentence: finding an topple topple to pillars obama’s topple obama’s pillars pillars topple \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-4f362218c73e>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('lstm.pth',map_location=device))\n"
          ]
        }
      ],
      "source": [
        "def generate(model, BOW, string='finding an ', strlen=10):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  print(f'input word : {string}')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for p in range(strlen):\n",
        "      words=torch.tensor([BOW[w] for w in string.split()],dtype=torch.long).to(device)\n",
        "\n",
        "      input_tensor=torch.unsqueeze(words[-2:],dim=0)\n",
        "      output=model(input_tensor)\n",
        "      output_word=(torch.argmax(output).cpu().numpy())\n",
        "      string += list(BOW.keys())[output_word]\n",
        "      string += \" \"\n",
        "\n",
        "  print(f'predicted sentence: {string}')\n",
        "\n",
        "model.load_state_dict(torch.load('lstm.pth',map_location=device))\n",
        "pred=generate(model,dataset.BOW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005F46oZ8ro4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
